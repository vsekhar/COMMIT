Random thoughts from other docs.

# Design thoughts

Two layers:

 * Verifiable log service: transparently timestamp and track independent log entries at scale
 * Verifiable chain service: transparently track small _serialized_ groups of log entries with expiry
 * Transaction protocol: construct a transaction by building a chain with votes

Log entries that are part of the same chain MUST be well ordered. Log entries that are part of different chains may be lazily ordered. As a result, the log service need not guarantee strict global (external) ordering.

> Intuition: if a group of participants begins two separate transactions, this starts two serialized but independent chains. As a result, one cannot offer any guarantees about which transaction completed first. In particular, the second transaction cannot depend on the resolution or expiry of the first transaction. If this dependency exists, the clients must include them as part of the same chain.

Log service data model:

  + Chain ID (specified by client or randomly generated by server)
    + Data hash (hash of data provided by client when creating record and server-generated salt)
    + Timestamp
    + Record ID (hash of full record including chain ID, and chain predecessor)

For a new Chain ID, no predecessor is used and the timestamp can be locally determined (no ordering guarantees for a new chain relative to other records). Subsequent records are added to a new chain transactionally, with a timestamp that is also locally-determined, but must be greater than the timestamp of the previous record).

Records can be lazily formed into a Merkle tree for verification. Merklization provides ordering only within chains. Merklization may place into the Merkle tree two records in different chains in the "wrong" order (violating timestamp order). Merklization MUST place any two records in the same chain in the "right" order (respecting timestamp order). This property is auditable in the log.

Records within a chain are externally consistent. No guarantees are provided for records between chains. Specifically, records between chains may be seen to appear in the wrong order in the log. For example, if a client creates record A in its own chain, waits for that operation to return success, and then creates record B in its own chain, it may be the case that record B appears in the log before record A. If the client wanted to depend on an ordering between the records, it must use the same chain.

> Is this a log-backed verifiable map? Key: ChainID, Value: full document, History: past versions of document and their timestamps. Maybe with a special AppendAtEnd operation rather than full Read-Modify-Write transactions.

> Log entries need to be (globally) chained in order to verify the timestamps are monotonic. This is distinct from the (local) chaining inherent to building out a transaction record. In other words, verifying timestamps is equivalent to building a massive single global chain. This is the root of the scalability problem.

-----

It consists of the following components:

 * Notary: a trust-nothing verifiable notary and timestamping service
 * Button: a timed atomic decision primitive
 * Consensus: a consensus protocol for multiple parties to agree on some action

## Notary

A notarization consists of three stanzas:

 1. A `ticket` generated by infrastructure
 1. A `document` generated by the client
 1. A `certificate` generated by infrastructure

Infrastructure generates and sends to the client a `ticket` with the following logical structure:

```http
X-Notary-Ticket-Timestamp-Range: <UTC nanosecond timestamp earliest>, <UTC nanosecond timestamp latest>
X-Notary-Ticket-Predecessor: <64 byte hash of "previous" ticket(s)>, ...
X-Notary-Ticket-Hash: <64 byte hash covering Timestamp-Range and Ticket-Predecessor(s) in order>
```

The infrastructure logs `Ticket-Hash` and can prove any such hash on request.

The client generates a `document` and sends it to the server with the following logical structure:

```http
X-Notary-Document-Timestamp: <UTC nanosecond timestamp within range of Timestamp-Range>
X-Notary-Document-Data: <64 bytes base58 encoded>
```

The server returns a `certificate`:

```http
X-Notary-Document-Hash: <64 byte hash covering Ticket-Hash, Timestamp, Data, Salt>
X-Notary-Certificate-Predecessor: <64 byte hash of "previous" certificate(s)>, ...
X-Notary-Certificate-Hash: <64 byte hash covering Document-Hash, and Certificate-Predecessor(s) in order>
```

The infrastructure logs `Certificate-Hash` and can prove any such hash on request.

The client can locally verify `Hash-Certificate`. The client can persist and pass around the full notarization to prove the validity of some document. Readers can locally validate that `Document-Hash` covers the document in question, and that `Certificate-Hash` covers a validly-constructed timetsamp as well as `Document-Hash`. Readers can remotely validate `Certificate-Hash` in the notary log.

Tickets are stored and logged in a verifiable log published by the infrastructure. Documents and certificates are stored in a separate log. Every certificate maps to and consumes one document, and every document maps to and consumes one ticket. However, some tickets can go permanently unusued, and some documents can go permanently uncertified (if for example it never makes it back to the infrastructure to be logged and certified).

The full notarization can be internally validated using the hashes. In addition, `Hash-Ticket`, `Hash-Document` and `Hash-Certificate` can be externally validated in the appropriate logs.

```http
X-Notary-Ticket-Timestamp-Range: <UTC nanosecond timestamp earliest>, <UTC nanosecond timestamp latest>
X-Notary-Ticket-Predecessor: <64 byte hash of "previous" ticket(s)>, ...
X-Notary-Ticket-Hash: <64 byte hash covering Timestamp-Range and Ticket-Predecessor(s) in order>
X-Notary-Document-Salt: <64 bytes base58 encoded>
X-Notary-Document-Timestamp: <UTC nanosecond timestamp within range of Timestamp-Range>
X-Notary-Document-Data: <64 bytes base58 encoded>
X-Notary-Document-Hash: <64 byte hash covering Ticket-Hash, Timestamp, Data, Salt>
X-Notary-Certificate-Predecessor: <64 byte hash of "previous" certificate(s)>, ...
X-Notary-Certificate-Hash: <64 byte hash covering Document-Hash, and Certificate-Predecessor(s) in order>
```

The protocol has the following properties:

 * Infrastructure pre-commits to a timestamp range, reducing its ability to fudge timestamps
 * Infrastructure publishes timestamp range, and the range becomes part of the final notarization, making any bias in the timestamp ranges detectable and provable
 * Infrastructure performs commit-wait until the _client-selected_ `Timestamp`, not until server-generated `tt.Latest`, to ensure it will never give an overlapping `Timestamp-Range` to any causally-related request
 * Clients can pass around tickets or pass around records to detect or prove bias in `Timestamp-Range`s being handed out.
 * Clients can causally chain documents to prove causality violations in `Timestamp-Range`s.

### Fast/trusted mode

If a client trusts the infrastructure, the client can directly request a full notarization in one round trip by sending its data directly:

```http
X-Notary-Document-Data: <64 bytes base58 encoded>
```

The server then performs the above negotiation internally and returns the remaining fields to build a full notarization:

```http
X-Notary-Ticket-Timestamp-Range: <UTC nanosecond timestamp earliest>, <UTC nanosecond timestamp latest>
X-Notary-Ticket-Predecessor: <64 byte hash of "previous" ticket(s)>, ...
X-Notary-Ticket-Hash: <64 byte hash covering Timestamp-Range and Ticket-Predecessor(s) in order>
X-Notary-Document-Salt: <64 bytes base58 encoded>
X-Notary-Document-Timestamp: <UTC nanosecond timestamp within range of Timestamp-Range>
X-Notary-Document-Hash: <64 byte hash covering Ticket-Hash, Timestamp, Data, Salt>
X-Notary-Certificate-Predecessor: <64 byte hash of "previous" certificate(s)>, ...
X-Notary-Certificate-Hash: <64 byte hash covering Document-Hash, and Certificate-Predecessor(s) in order>
```

Note that in this case the server still generates (and logs) a ticket using the same format above. This ensures that notarizations obtained via the fast/trusted mode cannot be distinguished from those using paranoid mode above, and can still be part of any analysis of timestamp bias.

### Implementation

Internally, the notary generates a ticket using `tt.Earliest + 1*RTT` as the first timestamp and `tt.Latest + 3*RTT` for the second timestamp. Estimates for `RTT` can be a (large-ish) fixed value. If a client selects a later timestamp in this interval, the client's commit-wait will be longer. Clients will likely select the first timestamp.

### Open questions

 * Can clients re-order events since they have the power to choose the timestamps?
 * Earliest timestamp in the range has to be in the future. But causality?
 * Clients can collect tickets and redeem them in whatever order.
   * Shuold the infrastructure choose the ultimate timestamp as well, just pre-commit to the range?
   * Fail if ultimate timestamp cannot fit range? There could still be bias, but bias would be publishable.
> * Base-case attack: Client can get two tickets with the same range, create two causally chained documents (B=Hash(A)) and submit them in reverse timestamp order.
 * Delay attack: infrastructure publishes really big fixed ranges to everyone, biases itself as usual.
 * Delay attack: everyone may choose earliest timestamp, so delay attack morphs to a timestamp pre-commitment attack (which can be attacked by the client via ticket collection)
 * Can neither client nor infrastructure choose the timestamp?
   * Diffie-Hellman-ish negotiation?
   * Hash-based?
 * Diffie-Hellman negotiation to mix (timestamp_range, salt) with (user_data) and produce (timestamp, hash).
