-- Schema hierarchy:
-- + Log (Region, Prefix, SequenceNo)
--   + INDEXed on (DataSHA3512)
-- + Decisions
--   + Actions

-- ---------------------
-- Problems being solved
-- ---------------------
--
-- Merkle tree implementations like Trillian provide provable inclusion of log
-- entries in a way infrastructure cannot invisibly alter. However using
-- Trillian as a web-scale historical notary requires solving a few additional
-- problems:
--
--   - Timestamping entries to allow their global ordering to be checked without
--     consulting the log, where timestamps are:
--     - Verifiably monotonic
--     - Verifiably consistent (B=hash(A), B timestamp must be after that of A)
--     - Verifiably fair (Alice doesn't get her entries delayed more than Bob)
--   - Scaling write throughput to web scale
--
-- Timestamping and global ordering are achievable using public block chains at
-- the cost of write throughput. Throughput can be increased by operating
-- multiple independent verifiable data structures, at the cost of global
-- ordering (cannot know the relative order of events written to different
-- data structures).
--
-- We propose the "Merkle weave", an open data structure and service backed by
-- Spanner and TrueTime, that addresses these problems.

-- ---------------
-- Design overview
-- ---------------
--
-- The log is split by region to reduce user-facing latency and split by hash
-- prefix (to increase write throughput). Each (Region, Prefix) pair defines an
-- independent Merkle Mountain Range. An MMR is functionally similar to a Merkle
-- tree, but is sequentially and deterministically built up from the bottom.
--
-- The result is a set of MMRs in each region that accept sequential writes
-- (each write must transactionally consult the most recent previous one).
--
-- We can prove the inclusion of any entry within its Region- and
-- Prefix-specific MMR in the usual way by traversing up form an entry in the
-- MMR to the MMR's digest.
--
-- Multiple independent MMRs introduce flex into the system by potentially
-- allowing malicious infrastructure to manipulate write order across MMRs. To
-- guard against this, we interlock the MMRs so that the datastructure
-- comprising the set of all MMRs is nevertheless lockstep built-up from user
-- inputs alone.
--
-- Interlocking is achieved in two ways. First, at the bottom, leaf entries tie
-- together different MMRs using deterministic prefixing and hash chaining.
-- Second, MMRs are tied together when an overall digest is computed across all
-- MMRs for a region. These techniques are described in the relevant sections
-- below.

-- --------------------------------
-- Appending new entries to the log
-- --------------------------------
--
-- DataSHA3512 is first computed using the 64-byte value provided by the user
-- and the 64-byte salt generated by the server (and returned to the user). The
-- timestamp is the Spanner commit timestamp (also returned to the user).
--
-- To determine the SequenceNo and the RecordSHA3512, we first compute two
-- prefixes by splitting DataSHA3512 as follows:
--
--   Index:         0 1 2 3 4 5 6 ... == DataSHA3512[0:64]
--   Prefix:       |0 1|2 3|          == DataSHA3512[0:4]
--                 |0 1|   |4 5|
--                   |   ┌───┘
--   CrossPrefix:  |0 1|4 5|          == append(DataSHA3512[0:2], DataSHA3512[4:6])
--
-- Generally, for prefix length N (which must be even):
--
--   Prefix = DataSHA3512[0:N]
--   CrossPrefix = append(DataSHA3512[0:N/2], DataSHA3512[N:N+N/2])
--
-- This ensures that the two prefixes are entirely determined by the DataSHA3512
-- (which neither the user nor the infrastructure can unilaterally determine),
-- that CrossPrefix's position relative to Prefix is unpredictable, and yet that
-- CrossPrefix is "close" to Prefix in the key space (which can be exploited for
-- server locality when adding entries).
--
-- SequenceNo is one larger than the most recent SequenceNo in Prefix.
--
-- Based on the SequenceNo, the new entry can be either an MMR leaf or non-leaf.
--
-- Non-leaf entries build up the MMR, so their predecessors are determined in
-- the usual way within Prefix:
--
--     Predecessor1 = RightChild(SequenceNo) = SequenceNo - 1
--     Predecessor2 = LeftChild(SequenceNo)
--
-- Leaves don't typically have a predecessor in a Merkle Tree/MMR. We can thus
-- exploit leaf entries to fix two problems uniquely introduced by this scheme:
-- 1) ensuring timestamps are causal and monotonic; and 2) ensuring the set of
--  partially correct for the issues introduced by
-- splitting the tree into many prefixes. We do this by interlocking the leaf
-- with the most recent entry in CrossPrefix.
--
--     Predecessor1 = most recent entry in Prefix = SequenceNo - 1
--     Predecessor2 = most recent entry in CrossPrefix
--
-- In either the leaf or non-leaf case, a Predecessor may not exist (if the
-- prefix in question has no entries yet). In that case, the predecessor is just
-- ignored.
--
-- RecordSHA3512 is determined by hashing in order:
--
--   1) RecordSHA3512 and Timestamp of Predecessor1, if exists
--   2) RecordSHA3512 and Timestamp of Predecessor2, if exists
--   3) DataSHA3512
--
-- Writing a new leaf requires consulting a different prefix chain which may
-- reside in a different Spanner group. In a binary MMR, no more than half of
-- all entries are leaves. The proximity between Prefix and CrossPrefix means it
-- is likely both can be accessed in the same Spanner group. Distributed
-- transactions are thus only incurred for leaves whose prefixes straddle
-- Spanner groups. These slow writes are fairly distributed by (random) hash
-- prefix, not by user or region. This is possibly the optimal arrangement given
-- the requirement to interlock MMRs.
--
-- RollingSHA3512 holds a rolling hash of the entire prefix chain (as an MMR)
-- as of the moment when that entry was added. It is determined by hashing in
-- order:
--
--   1) RecordSHA3512 of all peaks to the left of the current entry, as computed
--      using the new entry's SequenceNo (there will typically be O(log2N) such
--      peaks, e.g. 20 @ 1M entries).
--   2) RecordSHA3512 of the new entry
--
-- --------------------------------------------
-- Creating a digest of the full (regional) log
-- --------------------------------------------
--
-- The log logically consists of 2^(8N) prefix chains that each comprise an MMR.
--
-- The prefix length N determines how many prefix chains are active in the
-- region: higher N increases write throughput but also increases the number of
-- digests that must be reported.
--
-- For N=2, assuming two servers/groups are involved in every write and 100 QPS
-- per Spanner group, we have an upper bound of 65,536/2 * 100 = 3.2M QPS per
-- region. Additional smaller regions can increase overall throughput.
--
-- The digest for a prefix chain can be obtained by consulting the last entry in
-- that chain, or any earlier entry representing the point in time of interest.
--
-- A digest of a prefix chain can be externally reported as:
--
--     <region>:<prefix>:<number of entries in chain>:<last timestamp>:<SHA3512>
-- e.g. NA:ZUU:15673:6546516847616981651689135662165804354685:2wYM0X+pRKcMp1WjriLOcmGQbZtL9Vt0JckBCu86eA9WuD6kHhh0T7+tXWHuTzPVju+oy6DeRoEJDTjqWXxpJQ
--
-- The binary values <prefix> and <SHA3512> are URL-safe base64-encoded (RFC
-- 4648 §5) with no padding. Timestamp is encoded as nanoseconds from
-- 0001-01-01 00:00:00 UTC.
--
-- The digest of a full (regional) log can be computed by gathering digests of
-- prefix chains at some point in time. This is best done by reading some time
-- in the past. Spanner ensures we get a consistent view of the entire log
-- globally when performing this read.
--
-- The digest of a regional log can be reported as:
--
--      <region>:<last timestamp>:<SHA3512>
-- e.g. NA:6546516847616981651689135662165804354685:9w/cdNba+DmCdXnIAhhF/c4W2fDTUT1reavvujhzX1hOzfSZ2hyLz4KdlLkzMilXd6Qqh19f613YYNs4hpZ4Tw
--
-- The digest of the global log can be assembed from regional logs and reported
-- as:
--
--       <last timestamp>:<SHA3512>
-- e.g.  6546516847616981651689135662165804354685:Lh7Zv0z6McRkZmmlgCUznzbIastGxKHGDH8qOdOG5w0QIGDsOZmB19SYb53TH56gaEgm481o6acodXTVH0DZNg
--
-- Prefix, regional and global digests are computed and signed by the
-- infrastructure.
--
-- This digest is signed by the infrastructure, and the digest and signature are
-- hashed and themselves committed to the log. This means that anyone in
-- posesssion of a prior digest can request a proof that that digest was
-- included in any future one. The digest, its signature and the DataSHA3512 and
-- salt of its commitment are published.
--
-- TODO: What if multiple possible digests were created and committed and we
-- later shift around which one we claim is real? Signature solves that?
--
-- We can recreate this digest at any time in the future by doing time-specific
-- reads in the past at the desired timestamp: Spanner guarantees we will see an
-- externally consistent view of the database as it was at that time in the
-- past. Verifiers can do this themselves offline by simply filtering out
-- entries with timestamps later than the digest timestamp.
--
-- Creating digests makes heavy use of caching of the following entries:
--
--   * The RecordSHA3512 and Timestamp of the peaks of a prefix chain
--     * older/taller peaks remain among an MMR's peaks for a long time
--     * new tall peaks are known by the their SequenceNo, can be pre-emptively
--       cached
--   * The digest of each prefix chain at a given SequenceNo and Timestamp
--   * The digest of the log at a given Timestamp
--
-- -----------------------------
-- Proving inclusion of an entry
-- -----------------------------
--
-- Users request proofs of an entry by its DataSHA3512.
--
-- TODO: What is N? How does it evolve? Ok to start with some obscenely high
-- number? We're just computing predecessors for leaves, trees are otherwise
--
-- ----------------------------------------------
-- Proving inclusion of one digest in a later one
-- ----------------------------------------------
--
-- Within a prefix, this is done in the normal way.
--
-- TODO: across all prefixes? across all regions? these are not assembled into a
-- tree, so compact proofs are not available... 

-- NB: Cannot have a table higher in the hierarchy than Log (e.g. "Prefixes")
-- since it would limit Log to 4GB per prefix (min. 250k prefixes to hold 1PB).

-- Write new values spread across prefixes, which requires also quickly looking
-- up the most recent value to compute the new SequenceNo and recent and prior
-- values when computing RollingSHA3512.
CREATE TABLE Log (
    Region          STRING(MAX) NOT NULL,
    Prefix          BYTES(64) NOT NULL,
    SequenceNo      INT64 NOT NULL,
    DataSHA3512     BYTES(64) NOT NULL,
    RecordSHA3512   BYTES(64),
    RollingSHA3512  BYTES(64),
    Timestamp       TIMESTAMP NOT NULL OPTIONS (allow_commit_timestamp=true),
) PRIMARY KEY (Region, Prefix, SequenceNo DESC);

-- Memcache:
--
--   (Region, Prefix, SequenceNo) --> (RecordSHA3512, Timestamp)
--   (Region, Prefix, SequenceNo) --> RollingSHA3512
--   (Region, Timestamp) --> N:last_timestamp:hash

-- Lookup by timestamp is done by simply reading in the past. No index needed.

-- Lookup a DataSHA3512 when responding to user queries.
--
-- This index stores Timestamp so that we can access Log via a read in the past.
-- Making this unique guards against the exceedingly unlikely possibility of
-- collision.
CREATE UNIQUE INDEX EntriesByDataSHA3512 ON Log (DataSHA3512)
  STORING (Timestamp);

-- TODO: Decision table, actions table (interleave in decision table)
