Any distributed aggregation system will insert randomness into the stream of
events and how they are packed. This is true whether an asynchronous (e.g.
PubSub) or synchronous (e.g. blocking RPCs) method is used. Asynchronous
methods perform their own queuing and batching. Synchronous methods
effectively do the same with the requests themselves. A malicious service
provider could use these various sources of uncertainty to hide manipulations
of the stream of events.

To prevent this, a pack must capture the result of a particular set of server
choices and/or asynchronous events that occurred in the system. It is not
important what choices or events lead to a particular packing, only that once
a packing is produced, it is ordered, permanent and verifiable.

Since these are also the very same properties the service itself provides to
its clients, it can achieve these properties for its own packs by notarizing
them. Since the service is acting as its own client when notarizing packs,
packs themselves store the salt as well as their notarization. This allows
subsequent readers of the packs to validate their notarizations.

Though conceptually similar, notarizations of packs differ in their choice
of prefixes. Whereas client notarizations are assigned a fixed-length prefix
based on their notarization hash, pack notarizations are assigned prefixes
that are truncations of the prefix of the pack being notarized. Only pack
notarizations, generated by the service itself, are given the ability to
notarize to shorter (i.e. "higher-level") prefix chains.

This means there will ultimately be a prefix chain with no prefix. This is
the "top-level" prefix chain.

 Naming schemes:
 (sb64 == sortablebase64 (11 characters))

   Entries: /entries/hex(prefix)-ts:sb64(ts).entry
    - webserver timestamps requests from utime
    - webserver writes (with DoesNotExist) one .entry file for each request
      - timestamps must be unique within a prefix
      - if entry exists, retimestamp and retry
    - entry sent into "entries" topic with ordering_key=prefix
    - entry and timestamp returned to customer after commit-wait

   Canonical packs: /packs/hex(prefix)-sb64(seq_no).pack
    - packer probes /packs/* to find latest seq_no and last_ts in last pack
      for that prefix
    - packer subscribes to entries and is given a batch of entries with same prefix
    - packer acks and discards any messages with ts < last_ts
      - these were packed, but may not have been Ack'd
    - packer accumulates count(entries) and max(ts) of all entries
    - packer waits until count > N or max(ts) < now - timeout
    - packer sets max_ts = min(max_ts, uTime.Earliest)
      - ensures max_ts is in the past, i.e. no webserver will timestamp and
        save an entry with ts < max_ts on this prefix
    - packer reads entries from /entries from last_ts until max_ts
      - use startOffset in list API to start from last_ts
    - packer forms a batch of entries with last_ts < ts < max_ts
    - packer sorts entries by timestamp
    - packer writes a pack (with DoesNotExist)
      - cannot put timestamp range in name for this reason
    - packer acks all messages in the pack

   Index packs: /index/hex(prefix)-sb64(seq_no)-ts:sb64(first_ts):sb64(last_ts).index
    - Empty objects whose names are used to find seq_no of a pack that
      contains an entry with a given timestamp.
    - CAVEAT: timestamps may not be well ordered, pubsub ordering is based on
      an arrival order that is not observable to publishers or subscribers

Alternatives considered:

Rolling hash and repacking: use a rolling hash so that packs can be
   repacked without affecting hashes. This doesn't work because the tree
   of MMRs requires a fixed chunking. Each next pack needs to be seq_no+1
   from the previous one (in order for DoesNotExist write condition to
   work). Can't drop MMR since then we no longer have O(log2(N)) sized
   proofs all the way to the root.

   The purpose of notarizing packs is to capture and commit to the outcome
   of the asynchronous process of bundling entries together. Repacking
   goes against this and makes everything sequential again.

   Repacking is also not great since it increases the amount of data to
   download when looking up an entry. With 16 entries and 64 byte hashes,
   a full pack currently takes ~1.5kB. However this could be mitigated by
   building index files and supporting range reads.
